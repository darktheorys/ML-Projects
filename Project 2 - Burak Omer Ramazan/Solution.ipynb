{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center>CMPE 462 - Project 2<br>Implementing an SVM Classifier<br>Due: May 18, 2020, 23:59</center></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Student ID1:** 2016400186\n",
    "* **Student ID2:** 2016400078\n",
    "* **Student ID3:** 2016400003"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "In this project, you are going to implement SVM. For this purpose, a data set (data.mat) is given to you. You can load the mat dataset into Python using the function `loadmat` in `Scipy.io`. When you load the data, you will obtain a dictionary object, where `X` stores the data matrix and `Y` stores the labels. You can use the first 150 samples for training and the rest for testing. In this project, you will use the software package [`LIBSVM`](http://www.csie.ntu.edu.tw/~cjlin/libsvm/) to implement SVM. Note that `LIBSVM` has a [`Python interface`](https://github.com/cjlin1/libsvm/tree/master/python), so you can call the SVM functions in Python. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io as sp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "data = sp.loadmat(\"data.mat\")\n",
    "X = data.get(\"X\")\n",
    "y = data.get(\"Y\")\n",
    "X_train, y_train = X[:150], y[:150].flatten()\n",
    "X_test, y_test = X[150:], y[150:].flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1 - 30 pts\n",
    "\n",
    "Train a hard margin linear SVM and report both train and test classification accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 74.6667% (112/150) (classification)\n",
      "Accuracy = 77.5% (93/120) (classification)\n"
     ]
    }
   ],
   "source": [
    "from libsvm.svmutil import *\n",
    "prob = svm_problem(y_train, X_train)\n",
    "params = svm_parameter(\"-t 0 -c 1e8\")\n",
    "model = svm_train(prob, params)\n",
    "p_label, p_acc, p_val = svm_predict(y_train, X_train, model)\n",
    "p_label, p_acc, p_val = svm_predict(y_test, X_test, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2 - 40 pts\n",
    "\n",
    "Train soft margin SVM for different values of the parameter $C$, and with different kernel functions. Systematically report your results. For instance, report the performances of different kernels for a fixed $C$, then report the performance for different $C$ values for a fixed kernel, and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>linear</th>\n",
       "      <th>sv_l</th>\n",
       "      <th>polynomial</th>\n",
       "      <th>sv_p</th>\n",
       "      <th>rbf</th>\n",
       "      <th>sv_r</th>\n",
       "      <th>sigmoid</th>\n",
       "      <th>sv_s</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>C-Values</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.01</th>\n",
       "      <td>84.1667</td>\n",
       "      <td>118</td>\n",
       "      <td>58.3333</td>\n",
       "      <td>141</td>\n",
       "      <td>58.3333</td>\n",
       "      <td>140</td>\n",
       "      <td>58.3333</td>\n",
       "      <td>140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.16</th>\n",
       "      <td>85.0000</td>\n",
       "      <td>69</td>\n",
       "      <td>58.3333</td>\n",
       "      <td>141</td>\n",
       "      <td>84.1667</td>\n",
       "      <td>117</td>\n",
       "      <td>84.1667</td>\n",
       "      <td>115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.31</th>\n",
       "      <td>83.3333</td>\n",
       "      <td>67</td>\n",
       "      <td>84.1667</td>\n",
       "      <td>141</td>\n",
       "      <td>84.1667</td>\n",
       "      <td>103</td>\n",
       "      <td>85.0000</td>\n",
       "      <td>98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.46</th>\n",
       "      <td>85.0000</td>\n",
       "      <td>63</td>\n",
       "      <td>79.1667</td>\n",
       "      <td>134</td>\n",
       "      <td>83.3333</td>\n",
       "      <td>92</td>\n",
       "      <td>84.1667</td>\n",
       "      <td>89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9.31</th>\n",
       "      <td>81.6667</td>\n",
       "      <td>51</td>\n",
       "      <td>80.8333</td>\n",
       "      <td>85</td>\n",
       "      <td>77.5000</td>\n",
       "      <td>74</td>\n",
       "      <td>81.6667</td>\n",
       "      <td>56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9.46</th>\n",
       "      <td>81.6667</td>\n",
       "      <td>51</td>\n",
       "      <td>80.8333</td>\n",
       "      <td>85</td>\n",
       "      <td>77.5000</td>\n",
       "      <td>74</td>\n",
       "      <td>81.6667</td>\n",
       "      <td>56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9.61</th>\n",
       "      <td>81.6667</td>\n",
       "      <td>51</td>\n",
       "      <td>80.8333</td>\n",
       "      <td>85</td>\n",
       "      <td>77.5000</td>\n",
       "      <td>74</td>\n",
       "      <td>80.8333</td>\n",
       "      <td>56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9.76</th>\n",
       "      <td>81.6667</td>\n",
       "      <td>51</td>\n",
       "      <td>80.8333</td>\n",
       "      <td>85</td>\n",
       "      <td>77.5000</td>\n",
       "      <td>74</td>\n",
       "      <td>81.6667</td>\n",
       "      <td>57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9.91</th>\n",
       "      <td>81.6667</td>\n",
       "      <td>51</td>\n",
       "      <td>80.8333</td>\n",
       "      <td>84</td>\n",
       "      <td>77.5000</td>\n",
       "      <td>74</td>\n",
       "      <td>84.1667</td>\n",
       "      <td>55</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>68 rows Ã— 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           linear sv_l polynomial sv_p      rbf sv_r  sigmoid sv_s\n",
       "C-Values                                                          \n",
       "0.01      84.1667  118    58.3333  141  58.3333  140  58.3333  140\n",
       "0.16      85.0000   69    58.3333  141  84.1667  117  84.1667  115\n",
       "0.31      83.3333   67    84.1667  141  84.1667  103  85.0000   98\n",
       "0.46      85.0000   63    79.1667  134  83.3333   92  84.1667   89\n",
       "...           ...  ...        ...  ...      ...  ...      ...  ...\n",
       "9.31      81.6667   51    80.8333   85  77.5000   74  81.6667   56\n",
       "9.46      81.6667   51    80.8333   85  77.5000   74  81.6667   56\n",
       "9.61      81.6667   51    80.8333   85  77.5000   74  80.8333   56\n",
       "9.76      81.6667   51    80.8333   85  77.5000   74  81.6667   57\n",
       "9.91      81.6667   51    80.8333   84  77.5000   74  84.1667   55\n",
       "\n",
       "[68 rows x 8 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kernel_functions = [\"linear\", \"polynomial\", \"rbf\", \"sigmoid\"]                          # kernel function list\n",
    "sv_number = [[\"\"],[\"\"], [\"\"], [\"\"]]\n",
    "df = {\"linear\":[\"\"], \"sv_l\":[\"\"],  \"polynomial\":[\"\"], \"sv_p\":[\"\"], \"rbf\":[\"\"], \"sv_r\":[\"\"], \"sigmoid\":[\"\"], \"sv_s\":[\"\"] } # dataframe construction to visualize\n",
    "c_values = [\"C-Values\"]\n",
    "for kernel_function in range(len(kernel_functions)):        # for all kernel function given above\n",
    "    for c in range(1,1001,15):                              # for some c values\n",
    "        parameters = svm_parameter(\"-t {} -c {}\".format(kernel_function, c/100))           # parameterize\n",
    "        model = svm_train(prob, parameters)                                                # train\n",
    "        p_label, p_acc, p_val = svm_predict(y_test, X_test, model, \"-q\")                   # accuracy\n",
    "        df[kernel_functions[kernel_function]].append(\"{:.4f}\".format(p_acc[0]))            # append to df\n",
    "        sv_number[kernel_function].append(len(model.get_SV()))\n",
    "        if c/100 not in c_values:\n",
    "            c_values.append(c/100)\n",
    "[df[\"sv_l\"], df[\"sv_p\"], df[\"sv_r\"], df[\"sv_s\"]]  = sv_number\n",
    "pd.DataFrame(df, c_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3 - 15 pts\n",
    "\n",
    "Please report how the number of support vectors changes as the value of $C$ increases (while all other parameters remain the same). Discuss whether your observations match the theory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* In theory, as the value of C increses, the number of support vectors decrease, and accuracy of the model up to a c-value. \n",
    "* We know that if C is very high, model can turn into hard margin svm, if C is very low, model can ignore the error term and accuracy can be low. \n",
    "* The decrease in support vectors means that there are less vector that are supporting the margin. This can be because of we apply some regularization called soft margin, we ignore some support vectors and increase the fatness. Therefore, the number of supporting vectors decrease. \n",
    "* The decrease in support vector can be seen clearly from previous figure, which is said also in theory, so correlation make our work right.\n",
    "  * We see that, with polynomial, rbf and sigmoid we see many error with small C value. As C increases, they reach a top point and then start to decrease again. This is because it went to hard margin svm and lost some regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4 - 15 pts\n",
    "\n",
    "Please investigate the changes in the hyperplane when you remove one of the support vectors, vs., one data point that is not a support vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(58, 13)\n",
      "(55, 13)\n",
      "(58, 13)\n"
     ]
    }
   ],
   "source": [
    "def getHyperPlane(sv_dict, col, model):\n",
    "    sv_hyperplane = np.zeros((len(sv_dict),col))     # hyperplane matrix, initiated \n",
    "    j = 0 \n",
    "    for each in sv_dict:                             # for each support vector\n",
    "        for i in range(col):                         # for each attribute\n",
    "            if i+1 in each:                           # if (i+1) labeled element in support vector dictionary, we save\n",
    "                sv_hyperplane[j][i] = each.get(i+1)\n",
    "        j+=1\n",
    "    return (model.get_sv_coef() * sv_hyperplane)        # by multiplying w * H we obtain, real hyperplane Matrix\n",
    "\n",
    "row, col = np.shape(X_train)\n",
    "\n",
    "model = svm_train(prob, \"-t 0\")                          # training model using linear kernel\n",
    "sv_hyperplane = getHyperPlane(model.get_SV(), col, model)               \n",
    "\n",
    "#####################  w of default\n",
    "\n",
    "sv_to_delete_dict = model.get_SV()[1]                         # we choose 1st support vector to delete from our data\n",
    "sv_to_delete_list = np.zeros(col)\n",
    "for i in range(col):                                          # for each of attributes, we construct sv list from sv dict.\n",
    "    if i+1 in sv_to_delete_dict:\n",
    "        sv_to_delete_list[i] = (sv_to_delete_dict.get(i+1))\n",
    "ind = np.where((X_train == sv_to_delete_list).all(axis=1))[0][0]    # finding indice to delete from dataset\n",
    "\n",
    "X_train_sv_deleted = np.delete(X_train, ind, axis = 0)              # deleting sv from datased\n",
    "y_train_sv_deleted = np.delete(y_train, ind)                        # deleting corresponding label\n",
    "prob_sv_deleted = svm_problem(y_train_sv_deleted, X_train_sv_deleted)        # constructing svm problem with new dataset\n",
    "model_sv_deleted = svm_train(prob_sv_deleted, \"-t 0\")                        # training\n",
    "\n",
    "sv_deleted_hyperplane = getHyperPlane(model_sv_deleted.get_SV(), col, model_sv_deleted)      \n",
    "\n",
    "######################### w of sv deleted\n",
    "\n",
    "indice = 0                                                 # we will search for a non-sv data point to delete\n",
    "for i in range(row):                                       # for each data point\n",
    "    normal_point = X_train[i,:]          \n",
    "    sv_check = {}\n",
    "    for i in range(col):                                   # construct its sparse dictionary\n",
    "        if normal_point[i] != 0:\n",
    "            sv_check[i+1] = normal_point[i]\n",
    "\n",
    "    if sv_check not in model.get_SV():                     # if it is not in support vectors, we found one\n",
    "        indice = i\n",
    "        break\n",
    "\n",
    "X_train_nm_deleted = np.delete(X_train, indice, axis = 0)   # delete non - sv\n",
    "y_train_nm_deleted = np.delete(y_train, indice)             # delete non - sv\n",
    "prob_nm_deleted = svm_problem(y_train_nm_deleted, X_train_nm_deleted)         # new problem\n",
    "model_nm_deleted = svm_train(prob_nm_deleted, \"-t 0\")                         # new training\n",
    "\n",
    "nm_deleted_hyperplane = getHyperPlane(model_nm_deleted.get_SV(), col, model_nm_deleted)        # hyperplane\n",
    "\n",
    "############################ w or data point deleted\n",
    "\n",
    "print(np.shape(nm_deleted_hyperplane))\n",
    "print(np.shape(sv_deleted_hyperplane))\n",
    "print(np.shape(sv_hyperplane))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Number of support vectors decreased not only by the deleted one, but 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bonus Task - 10 pts\n",
    "\n",
    "Use Python and [CVXOPT](http://cvxopt.org) QP solver to implement the hard margin SVM. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'cvxopt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-0e2655992de9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#Code to run cvxopt is mainly taken from https://xavierbourretsicotte.github.io/SVM_implementation.html\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m#where it is mainly taken from doccumentation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mcvxopt\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatrix\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mcvxopt_matrix\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mcvxopt\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msolvers\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mcvxopt_solvers\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'cvxopt'"
     ]
    }
   ],
   "source": [
    "#Code to run cvxopt is mainly taken from https://xavierbourretsicotte.github.io/SVM_implementation.html \n",
    "#where it is mainly taken from doccumentation\n",
    "from cvxopt import matrix as cvxopt_matrix\n",
    "from cvxopt import solvers as cvxopt_solvers\n",
    "\n",
    "#Initializing values and computing H. Note the 1. to force to float type\n",
    "row, col = X_train.shape\n",
    "y = y_train.reshape(row,1) * 1.\n",
    "X_dash = y * X_train\n",
    "H = X_dash @ X_dash.T * 1.\n",
    "\n",
    "#Converting into cvxopt format\n",
    "P = cvxopt_matrix(H)\n",
    "q = cvxopt_matrix(-np.ones((row, 1)))\n",
    "G = cvxopt_matrix(-np.eye(row))\n",
    "h = cvxopt_matrix(np.zeros(row))\n",
    "A = cvxopt_matrix(y.reshape(1, -1))\n",
    "b = cvxopt_matrix(np.zeros(1))\n",
    "\n",
    "#Setting solver parameters (change default to decrease tolerance) \n",
    "cvxopt_solvers.options['show_progress'] = False\n",
    "cvxopt_solvers.options['abstol'] = 1e-10\n",
    "cvxopt_solvers.options['reltol'] = 1e-10\n",
    "cvxopt_solvers.options['feastol'] = 1e-10\n",
    "\n",
    "#Run solver\n",
    "sol = cvxopt_solvers.qp(P, q, G, h, A, b)\n",
    "alphas = np.array(sol['x'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'alphas' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-b4723a57dd19>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#w parameter in vectorized form\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mw\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0malphas\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m \u001b[1;33m@\u001b[0m \u001b[0mX_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m#Selecting the set of indices S corresponding to non zero parameters\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mS\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0malphas\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1e-4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'alphas' is not defined"
     ]
    }
   ],
   "source": [
    "#w parameter in vectorized form\n",
    "w = ((y * alphas).T @ X_toy).reshape(col,1)\n",
    "\n",
    "#Selecting the set of indices S corresponding to non zero parameters\n",
    "S = (alphas > 1e-4).flatten()\n",
    "b = (y[S] - np.dot(X_toy[S], w))[0]\n",
    "\n",
    "print('w = ', w)\n",
    "print('b = ', b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "print ( np.sum(np.sign( X_toy @ w + b) == y) / len(y)) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
